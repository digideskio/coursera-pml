{"name":"Coursera-pml","tagline":"Practical Machine Learning with coursera","body":"# coursera-pml\r\nPractical Machine Learning with coursera\r\n\r\n## Data analysis\r\n\r\nWe begin with analysing the data manually. The training dataset contains 19.622 observations of 160 variables. The test dataset consists of 20 rows.\r\n\r\nThe output parameter is named `classe` and is a factor with 5 values.\r\n\r\n```R\r\n> str(train$classe)\r\n Factor w/ 5 levels \"A\",\"B\",\"C\",\"D\",..: 1 1 1 1 1 1 1 1 1 1 ...\r\n> summary(train$classe)\r\n   A    B    C    D    E \r\n5471 3718 3352 3147 3528 \r\n```\r\n\r\nJust looking at the data, there seems to be quite a lot of empty cells and NAs. Good - we'll be able to trim down the number of colums before we actually feed the data to the training algorithms.\r\n\r\nAnother interesting observation is that every ~24 rows there seems to be some kind of a summary row copy-pasted from excel - it has string values of `#DIV/0!`, and in general it has very different data than the _regular_ rows - it's marked with `new_window=yes`. This makes me think that it would be wise to divide the data into two parts based on the `new_window` column values. Furthermore we could try to build two separate models for the two parts. But for now, since it's only ~400 rows out of 19k (unnecessary clutter?), we'll skip these rows in the processing.\r\n\r\n\r\n```R\r\ntable(pml.training$new_window)\r\n\r\n   no   yes \r\n19216   406 \r\n```\r\n\r\nColumn `X` is just an ID, so it won't be used as a feature.\r\n\r\nColumn `user_name` is a factor with 6 values. The assignment is about analysing whether a physical excercise is performed correctly or incorrectly, and classifying the type of errors. Although we could use this feature as the test set has exactly the same factors for this column, the algorithm shouldn't be dependent on the `user_name` of the person doing excercises, should be able to predict independently of who does the excercises.\r\n\r\n```R\r\ntable(pml.training$user_name)\r\n\r\n  adelmo carlitos  charles   eurico   jeremy    pedro \r\n    3892     3112     3536     3070     3402     2610 \r\n```\r\n\r\nThere are three timestamp columns, that we might consider. Idea - figure out for how long the person has been excercising already - maybe there's a correlation: fatigue vs error type.\r\n\r\n`num_window` seems to be some metadata related to the capturing devices. Discarding.\r\n\r\nNow there seem to be a lot of columns related to the sensor measurements - still there's so many columns it's hard to go thru them all an filter manually. The general intuition is that we should be using features related to `arm`s, `belt`s, `dumbbell`s and `forearm`s. The rest of columns look promising for training models and running predictions.\r\n\r\n## Data preprocessing & Feature selection\r\n\r\nWe begin with the low-hanging fruits. Let's follow the intuition, let's run some naive preprocessing to clean the data, and see what kind of results we get out of it.\r\n\r\nRegarding the cleaning - I decided to follow the approach of discarding all columns that have more than 75% empty cells (empty means `''`, `NA` or `#DIV/0!`). Most probably, these columns won't add much value to the final output.\r\n\r\n```R\r\ny <- c(\"classe\")\r\n# discard features that don't seem valuable\r\nxs <- setdiff(colnames(pml.training), c(\"X\", \"user_name\", \"new_window\", \"num_window\", \"raw_timestamp_part_1\", \"raw_timestamp_part_2\", \"cvtd_timestamp\", \"classe\"))\r\n\r\n# remove the summary rows\r\nfilterRows <- function(ds) {\r\n  ds[ds$new_window == \"no\",];\r\n}\r\n\r\ntrain <- filterRows(pml.training);\r\ntest <- filterRows(pml.testing);\r\n\r\n# drop columns that are mostly empty (NA, empty string or #DIV/0!)\r\nmostlyEmpty <- sapply(xs, function(x) sum(is.na(train[, x]) | train[, x] == \"\" | train[, x] == \"#DIV/0!\") > 0.75 * nrow(train))\r\nxs <- xs[!mostlyEmpty]\r\n\r\ntrain <- train[,c(xs, y)]\r\ntest <- test[,c(xs)]\r\n```\r\n\r\nAt this point, the numbers of potential features fell from 160 to 52. Seems like we just discarded a lot of crap :).\r\n\r\n```R\r\nxs\r\n [1] \"roll_belt\"            \"pitch_belt\"           \"yaw_belt\"             \"total_accel_belt\"     \"gyros_belt_x\"        \r\n [6] \"gyros_belt_y\"         \"gyros_belt_z\"         \"accel_belt_x\"         \"accel_belt_y\"         \"accel_belt_z\"        \r\n[11] \"magnet_belt_x\"        \"magnet_belt_y\"        \"magnet_belt_z\"        \"roll_arm\"             \"pitch_arm\"           \r\n[16] \"yaw_arm\"              \"total_accel_arm\"      \"gyros_arm_x\"          \"gyros_arm_y\"          \"gyros_arm_z\"         \r\n[21] \"accel_arm_x\"          \"accel_arm_y\"          \"accel_arm_z\"          \"magnet_arm_x\"         \"magnet_arm_y\"        \r\n[26] \"magnet_arm_z\"         \"roll_dumbbell\"        \"pitch_dumbbell\"       \"yaw_dumbbell\"         \"total_accel_dumbbell\"\r\n[31] \"gyros_dumbbell_x\"     \"gyros_dumbbell_y\"     \"gyros_dumbbell_z\"     \"accel_dumbbell_x\"     \"accel_dumbbell_y\"    \r\n[36] \"accel_dumbbell_z\"     \"magnet_dumbbell_x\"    \"magnet_dumbbell_y\"    \"magnet_dumbbell_z\"    \"roll_forearm\"        \r\n[41] \"pitch_forearm\"        \"yaw_forearm\"          \"total_accel_forearm\"  \"gyros_forearm_x\"      \"gyros_forearm_y\"     \r\n[46] \"gyros_forearm_z\"      \"accel_forearm_x\"      \"accel_forearm_y\"      \"accel_forearm_z\"      \"magnet_forearm_x\"    \r\n[51] \"magnet_forearm_y\"     \"magnet_forearm_z\"    \r\n```\r\n\r\n## Training model\r\n\r\nSince it's a classification problem, not all algorithms will fit. From my previous experience (e.g. kaggle competitions) Random Forests proved to be the most efficient for classification problems - so I'll start with that.\r\n\r\nAs usual - we need to know whether our model works and what level of accuracy we can get. For this purpose we apply cross validation, partitioning the dataset at 70% training and 30% testing.\r\n\r\n```R\r\ninTrain  <- createDataPartition(train[, y], p = 0.7, list=FALSE);\r\ntraining <- train[inTrain, c(y, xs)];\r\ntesting  <- train[-inTrain, c(y, xs)];\r\n\r\nmodel <- train(classe ~ ., data = training, method = \"rf\", trControl = trainControl(method=\"cv\"), numbers=3);\r\n\r\npredictions <- predict(model, testing);\r\nconfusionMatrix(predictions, testing[, y]);\r\n\r\n# variable importance\r\nplot(varImp(model, scale = FALSE))\r\n```\r\n\r\nThis gives very high accuracy already, much higher than the 'good-enough' level around 80%.\r\n\r\n```R\r\n> model\r\nRandom Forest \r\n\r\n13453 samples\r\n   52 predictor\r\n    5 classes: 'A', 'B', 'C', 'D', 'E' \r\n\r\nNo pre-processing\r\nResampling: Cross-Validated (10 fold) \r\n\r\nSummary of sample sizes: 12109, 12107, 12107, 12107, 12106, 12108, ... \r\n\r\nResampling results across tuning parameters:\r\n\r\n  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   \r\n   2    0.9911618  0.9888175  0.002509612  0.003176747\r\n  27    0.9913587  0.9890679  0.002350200  0.002973651\r\n  52    0.9849896  0.9810093  0.007077471  0.008957960\r\n\r\nAccuracy was used to select the optimal model using  the largest value.\r\nThe final value used for the model was mtry = 27. \r\n>\r\n> model$finalModel\r\n\r\nCall:\r\n randomForest(x = x, y = y, mtry = param$mtry, numbers = 3) \r\n               Type of random forest: classification\r\n                     Number of trees: 500\r\nNo. of variables tried at each split: 27\r\n\r\n        OOB estimate of  error rate: 0.71%\r\nConfusion matrix:\r\n     A    B    C    D    E class.error\r\nA 3823    4    1    0    2 0.001827676\r\nB   17 2577    9    0    0 0.009988475\r\nC    0    9 2330    8    0 0.007243289\r\nD    0    1   31 2169    2 0.015433500\r\nE    0    0    4    8 2458 0.004858300\r\n```\r\n\r\nConfusion matrix:\r\n\r\n```R\r\n> cm\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 1638    6    0    0    0\r\n         B    3 1098    5    0    0\r\n         C    0    5  997   22    3\r\n         D    0    6    3  922    3\r\n         E    0    0    0    0 1052\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9903          \r\n                 95% CI : (0.9874, 0.9927)\r\n    No Information Rate : 0.2847          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9877          \r\n Mcnemar's Test P-Value : NA              \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            0.9982   0.9848   0.9920   0.9767   0.9943\r\nSpecificity            0.9985   0.9983   0.9937   0.9975   1.0000\r\nPos Pred Value         0.9964   0.9928   0.9708   0.9872   1.0000\r\nNeg Pred Value         0.9993   0.9963   0.9983   0.9954   0.9987\r\nPrevalence             0.2847   0.1935   0.1744   0.1638   0.1836\r\nDetection Rate         0.2842   0.1905   0.1730   0.1600   0.1825\r\nDetection Prevalence   0.2853   0.1919   0.1782   0.1621   0.1825\r\nBalanced Accuracy      0.9984   0.9915   0.9929   0.9871   0.9972\r\n```\r\n\r\n### Other algorithms\r\n\r\nLater on I ran a bunch of tests with other models and parameters, e.g.:\r\n\r\n* Random Forests with PCA preprocessing,\r\n* R-Part with PCA\r\n* LDA2 with PCA\r\n* PAM with PCA\r\n\r\nBut none of them yields results as good as the pure `rf`. So, I'll stick with that.\r\n\r\n### Variable importance\r\n\r\nVariable importance plot shows that row_belt, pitch_forearm, yaw_belt and pitch_belt have the most impact on the predictions.\r\n\r\n![variable importance](./varImp.png?raw=true)\r\n\r\nOut of pure curiousity, I ran the training and predictions taking into consideration only 7 the most influential features:\r\n\r\n```R\r\nxs <- c(\"roll_belt\", \"pitch_forearm\", \"yaw_belt\", \"pitch_belt\", \"roll_forearm\", \"magnet_dumbbell_y\", \"magnet_dumbbell_z\")\r\n```\r\n\r\nInterestingly, the accuracy on CV turned out to be 98%, which is 1% less than with all 52 features.\r\n\r\n## Conclusion\r\n\r\nI seem to have got very decent results just by following the intuition and keeping things as simple as possible. At the beginning, when I was manually going through the data and analysing, I had a couple of ideas in the back of my head - e.g. including the rows with `new_window=yes` as a separate model, adding a fatigue feature (based on time passed since the beginning of excercising), but in the end I managed to get 99% estimated accuracy (based on cross validation) without actually implementing any of that.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}